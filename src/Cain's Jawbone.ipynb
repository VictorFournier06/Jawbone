{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token size: 384\n"
     ]
    }
   ],
   "source": [
    "# global imports (1 sec - 30 secs)\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from sentence_transformers import SentenceTransformer\n",
    "accelerator = Accelerator()\n",
    "model = accelerator.prepare(SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda\")) #all-MiniLM-L6-v2\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "# Get the max token size\n",
    "max_token_size = model.max_seq_length\n",
    "print(f\"Max token size: {max_token_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports (1 sec - 2 secs)\n",
    "import loader\n",
    "import distances\n",
    "import algos\n",
    "import data_processing as dp\n",
    "import classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear all memory of the GPU\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.reset_peak_memory_stats(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database\n",
    "\n",
    "The miscellaneous database consists of 20 copyright-free book from children's literature obtained from [The Project Gutenberg](https://www.gutenberg.org/ebooks/bookshelf/20).  \n",
    "The Tom Swift database consists of 27 books of the series Tom Swift by Victor Appleton obtained from [The Project Gutenberg](https://www.gutenberg.org/ebooks/search/?query=victor+appleton&submit_search=Go%21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing the database (\"Miscellaneous\", \"Tom Swift\") and the number of books to load (1 min)\n",
    "nb_books = 25\n",
    "mode = \"chunks\"\n",
    "all_sentences = loader.load_books(\"Tom Swift\", mode, max_token_size, tokenizer=model.tokenizer)\n",
    "#Randomly reorder the books\n",
    "random.shuffle(all_sentences)\n",
    "sentences = all_sentences[:nb_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the real object of my visit was to say this to you.\" The man approached still closer to Tom, and, in a lower voice, and one that could scarcely be heard, he fairly hissed: \"Don't go with Barcoe Jenks to seek the diamond makers!\" Then, before Tom could put out a hand to detain him, had the lad so wished, the man turned suddenly, and fairly ran from the shed. CHAPTER VI--MR. DAMON IS ON HAND The young inventor stood almost spellbound for a few moments. Then recovering himself he made a dash for the door through which the mysterious man had disappeared. Tom saw him sprinting down the road, and was half-minded to take after him, but a cooler thought warned him that he had better not. \"He may be one of those men who are on Mr. Jenks' trail,\" reasoned Tom, in which case it might not be altogether safe to attempt to stop him, and make him explain. Or he may be a lunatic, and in that case it wouldn't be altogether healthy to interfere with him. \"I'll just let him go, and tell Mr. Jenks about him when he comes to-night. But I must warn Rad never to let him in here again. He might damage the airship.\" Calling to the colored man, Tom pointed to the stranger, who was almost out of sight down the road, and said earnestly: \"Rad, do you see that fellow?\" \"I sho do, Massa Tom, but I sorter has t' strain my eyes t' do it. He's goin' laik my mule Boomerang does when he's comm' home t' dinnah.\" \"That's right, Rad. Well, never let that man set foot inside our fence again! If he comes, \n"
     ]
    }
   ],
   "source": [
    "#choose a random book and print 1 chunk\n",
    "r = random.randrange(0, len(sentences)) #not included\n",
    "r2 = random.randrange(0, len(sentences[r]))\n",
    "for i in range(r2, r2+1):\n",
    "    print(sentences[r][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Here we use Sentence-BERT to embed the sentences in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings of the sentences (1 min - 2 min)\n",
    "sentence_embedding = [model.encode(sentences[r]) for r in range(len(sentences))] #cannot stack because different number of pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing the distances\n",
    "Here we want to find an oprtimal order by maximizing the semantic proximity between neighboring sentences. We have $n!$ possible orderings, so we can't use brute force. Our problem is similar to the traveling salesman problem, which is NP-hard, so we can't solve it optimally, therefore we design some algorithms to try to find a local minimum instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default order            <lambda>            avg_consecutive_dist     0.3129834234714508\n",
      "default order            <lambda>            kendall_tau              1.0\n",
      "default order            <lambda>            avg_R_dist               0.0\n",
      "\n",
      "random_order             <lambda>            avg_consecutive_dist     0.4896382987499237\n",
      "random_order             <lambda>            kendall_tau              -0.07919191919191926\n",
      "random_order             <lambda>            avg_R_dist               1.0\n",
      "\n",
      "default order            insertion_sort      avg_consecutive_dist     0.380018413066864\n",
      "default order            insertion_sort      kendall_tau              0.6177777777777778\n",
      "default order            insertion_sort      avg_R_dist               0.88\n",
      "\n",
      "random_order             insertion_sort      avg_consecutive_dist     0.45770129561424255\n",
      "random_order             insertion_sort      kendall_tau              0.2707070707070707\n",
      "random_order             insertion_sort      avg_R_dist               0.98\n",
      "\n",
      "default order            greedy_sort         avg_consecutive_dist     0.3288816511631012\n",
      "default order            greedy_sort         kendall_tau              0.9769696969696969\n",
      "default order            greedy_sort         avg_R_dist               0.88\n",
      "\n",
      "random_order             greedy_sort         avg_consecutive_dist     0.434881329536438\n",
      "random_order             greedy_sort         kendall_tau              -0.1200000000000001\n",
      "random_order             greedy_sort         avg_R_dist               0.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run the local minimum algorithms on a subset of the sentences of the book and compare the permutation distances (1 sec - 4 secs)\n",
    "pairwise_dist = distances.pairwise_dist(sentence_embedding)\n",
    "distances2 = pairwise_dist[0][:100,:100]\n",
    "\n",
    "default_order, random_order = list(range(len(distances2))), np.random.permutation(len(distances2))\n",
    "\n",
    "for algo in [(lambda x, y: y), algos.insertion_sort, algos.greedy_sort]:\n",
    "    for order, order_name in [(default_order, \"default order\"), (random_order, \"random_order\")]:\n",
    "        for dist, dist_name in [(lambda o : distances.avg_consecutive_dist(o, distances2), \"avg_consecutive_dist\"), (lambda o : distances.kendall_tau(o, default_order), \"kendall_tau\"), (lambda o : distances.avg_R_dist(o, default_order), \"avg_R_dist\")]:\n",
    "            ordered = algo(distances2, order)\n",
    "            d = dist(ordered)\n",
    "            print(f\"{order_name:25}{algo.__name__:20}{dist_name:25}{d}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The insertion sort improves significantly the kendall tau compared to a random permutation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier: \n",
    "### Heuristics\n",
    "The two metrics discussed were the following: distance betweeen pages or probability that a page is before another.\n",
    "\n",
    "For the distance between pages, we don't have the ground truth so to train a model it's easier to start with a classifier that takes in two pages and outputs a probability that the first page is before the second.\n",
    "\n",
    "![Classifier Architecture](../img/Classifier_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155, 768)\n",
      "torch.Size([124, 768]) torch.Size([16, 768]) torch.Size([15, 768])\n"
     ]
    }
   ],
   "source": [
    "#split the sentences into discard, training, validation and testing sets keeping the order\n",
    "print(sentence_embedding[0].shape)\n",
    "sentences_train, sentences_val, sentences_test = dp.split_sentences(sentence_embedding, 1)\n",
    "print(sentences_train[0].shape, sentences_val[0].shape, sentences_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([367369, 1536]) torch.Size([6123, 1536]) torch.Size([5724, 1536])\n"
     ]
    }
   ],
   "source": [
    "#create the database of the pairs of a subset of sentences (30 sec - 1 min for 30 books and 100%)\n",
    "X_train, y_train = dp.create_database(sentences_train)\n",
    "X_val, y_val = dp.create_database(sentences_val)\n",
    "X_test, y_test = dp.create_database(sentences_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)     #size n x (n-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "input_dim = X_train[0].shape[0]\n",
    "output_dim = 1\n",
    "hidden_dim = 128\n",
    "learning_rate_list = [0.1, 0.01, 0.001, 0.0001]\n",
    "epochs_list = [10, 100, 1000]\n",
    "L2_alphas = [0, 0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the classifier\n",
    "network = classifier.Classifier(input_dim, hidden_dim, output_dim, accelerator)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#finetune the hyperparameters (10 minutes)\n",
    "best_loss, best_state, best_hyperparams = network.finetuning(learning_rate_list, epochs_list, L2_alphas, X_train, y_train, X_val, y_val, True)\n",
    "network.load_state_dict(best_state)\n",
    "print(\"best loss:\", best_loss)\n",
    "print(\"best learning rate:\", best_hyperparams[0])\n",
    "print(\"best epochs:\", best_hyperparams[1])\n",
    "print(\"best L2 alpha:\", best_hyperparams[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best values for the hyperparameters were found to be:\n",
    "- Number of epochs: 10 000\n",
    "- Learning Rate: 0.001\n",
    "- L2 Regularization: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the best hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "L2_alpha = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the classifier\n",
    "network = classifier.Classifier(input_dim, hidden_dim, output_dim, accelerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss 0.692900\n",
      "Epoch 100: train loss 0.496652\n",
      "Epoch 200: train loss 0.457075\n",
      "Epoch 300: train loss 0.427931\n",
      "Epoch 400: train loss 0.387018\n",
      "Epoch 500: train loss 0.342939\n",
      "Epoch 600: train loss 0.303770\n",
      "Epoch 700: train loss 0.271214\n",
      "Epoch 800: train loss 0.244176\n",
      "Epoch 900: train loss 0.221592\n",
      "Validation loss 0.457605\n"
     ]
    }
   ],
   "source": [
    "#train the network with the best hyperparameters (1 mins - 1.5 min)\n",
    "loss = network.train(X_train, y_train, X_val, y_val, epochs, learning_rate, L2_alpha, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the pairwise order predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values 5346\n",
      "Test BCE loss 0.482733\n",
      "Test accuracy 0.796483\n",
      "Test F1 0.799114\n",
      "Test AUC 0.796483\n"
     ]
    }
   ],
   "source": [
    "#test on the GPU\n",
    "y_pred = network(X_test.float().cuda())\n",
    "#BCE loss\n",
    "loss = network.loss_fn()(y_pred, y_test.float().cuda())\n",
    "#remove elements are diagonal elements\n",
    "indices = [i for i,v in enumerate(y_test) if v==0.5]\n",
    "y_pred_hole = torch.tensor([value for index, value in enumerate(y_pred) if not index in indices])\n",
    "y_test_hole = torch.tensor([value for index, value in enumerate(y_test) if not index in indices])\n",
    "#convert to numpy arrays and round predictions\n",
    "y_test_array, y_pred_array = y_test_hole.cpu().detach().numpy(), y_pred_hole.cpu().detach().numpy().round()\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y_test_array, y_pred_array)\n",
    "#F1 score\n",
    "F1 = f1_score(y_test_array, y_pred_array)\n",
    "#AUC score\n",
    "AUC = roc_auc_score(y_test_array, y_pred_array)\n",
    "\n",
    "print(\"Number of values %d\" % y_test_array.shape[0])\n",
    "print(\"Test BCE loss %f\" % loss.item())\n",
    "print(\"Test accuracy %f\" % accuracy)\n",
    "print(\"Test F1 %f\" % F1.item())\n",
    "print(\"Test AUC %f\" % AUC.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed a new book (2.5 sec)\n",
    "new_embedding = [model.encode(all_sentences[nb_books])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the pairwise page order in the new book using the network (2 secs)\n",
    "reduced_embedding = dp.split_sentences(new_embedding, 1, 1)[0]\n",
    "X2, y2 = dp.create_database(reduced_embedding)\n",
    "y2_pred = network(X2.float().cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the pairwise ordering predictions for a new book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values 24492\n",
      "Test BCE loss 0.576227\n",
      "Test accuracy 0.749347\n",
      "Test F1 0.751246\n",
      "Test AUC 0.749347\n"
     ]
    }
   ],
   "source": [
    "#BCE loss\n",
    "loss = network.loss_fn()(y2_pred, y2.float().cuda())\n",
    "#remove elements are diagonal elements\n",
    "indices = [i for i,v in enumerate(y2) if v==0.5]\n",
    "y2_pred_hole = torch.tensor([value for index, value in enumerate(y2_pred) if not index in indices])\n",
    "y2_hole = torch.tensor([value for index, value in enumerate(y2) if not index in indices])\n",
    "#convert to numpy arrays and round predictions\n",
    "y2_array, y2_pred_array = y2_hole.cpu().detach().numpy(), y2_pred_hole.cpu().detach().numpy().round()\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y2_array, y2_pred_array)\n",
    "#F1 score\n",
    "F1 = f1_score(y2_array, y2_pred_array)\n",
    "#AUC score\n",
    "AUC = roc_auc_score(y2_array, y2_pred_array)\n",
    "\n",
    "print(\"Number of values %d\" % y2_array.shape[0])\n",
    "print(\"Test BCE loss %f\" % loss.item())\n",
    "print(\"Test accuracy %f\" % accuracy)\n",
    "print(\"Test F1 %f\" % F1.item())\n",
    "print(\"Test AUC %f\" % AUC.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#convert the predictions to a pairwise probability matrix\n",
    "pairwise_probabilities = dp.flattened_to_matrix(y2_pred)\n",
    "#convert to a shifted antisymmetric matrix by averaging the predictions of the upper and lower triangular\n",
    "averaged_probabilities = dp.average_matrix(pairwise_probabilities)\n",
    "#test if the shifted matrix is asymmetric\n",
    "print(np.sum(averaged_probabilities-0.5 + np.transpose(averaged_probabilities-0.5))==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the min weight transitivity closure of the pairwise order graph using Floyd-Warshall\n",
    "min_closure = algos.order_from_pairwise(pairwise_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of the transitive predictions for the new book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values 24492\n",
      "Test accuracy 0.759758\n",
      "Test F1 0.756920\n",
      "Test AUC 0.759758\n"
     ]
    }
   ],
   "source": [
    "#convert directed edges to a list 0 and 1\n",
    "transitive_pred = dp.edges_to_pred(min_closure)\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y2_array, transitive_pred)\n",
    "#F1 score\n",
    "F1 = f1_score(y2_array, transitive_pred)\n",
    "#AUC score\n",
    "AUC = roc_auc_score(y2_array, transitive_pred)\n",
    "\n",
    "print(\"Number of values %d\" % transitive_pred.shape[0])\n",
    "print(\"Test accuracy %f\" % accuracy)\n",
    "print(\"Test F1 %f\" % F1.item())\n",
    "print(\"Test AUC %f\" % AUC.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute obtimal order the topological sort algorithm\n",
    "pred_order = algos.topological_sort(min_closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kendall tau: 0.520496\n",
      "R distance: 0.993631\n"
     ]
    }
   ],
   "source": [
    "#compare with the actual order of masked pages\n",
    "ground_truth_order = list(range(len(new_embedding[0])))\n",
    "print(\"Kendall tau: %f\" % distances.kendall_tau(pred_order, ground_truth_order))\n",
    "print(\"R distance: %f\" % distances.avg_R_dist(pred_order, ground_truth_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Transformer:\n",
    "### Heuristics \n",
    "Full transformer network that takes in pages and outputs an order, the architecture for this set-to-sequence model comes from [Set Transformer](https://arxiv.org/abs/1810.00825).\n",
    "\n",
    "DRAWING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
