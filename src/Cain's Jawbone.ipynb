{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token size: 384\n"
     ]
    }
   ],
   "source": [
    "# global imports (1 sec - 30 secs)\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from sentence_transformers import SentenceTransformer\n",
    "accelerator = Accelerator()\n",
    "model = accelerator.prepare(SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda\")) #all-MiniLM-L6-v2\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "# Get the max token size\n",
    "max_token_size = model.max_seq_length\n",
    "print(f\"Max token size: {max_token_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports (1 sec - 2 secs)\n",
    "import loader\n",
    "import distances\n",
    "import algos\n",
    "import data_processing as dp\n",
    "import classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear all memory of the GPU\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.reset_peak_memory_stats(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database\n",
    "\n",
    "The miscellaneous database consists of 20 copyright-free book from children's literature obtained from [The Project Gutenberg](https://www.gutenberg.org/ebooks/bookshelf/20).  \n",
    "The Tom Swift database consists of 27 books of the series Tom Swift by Victor Appleton obtained from [The Project Gutenberg](https://www.gutenberg.org/ebooks/search/?query=victor+appleton&submit_search=Go%21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing the database (\"Miscellaneous\", \"Tom Swift\") and the number of books to load (1 min)\n",
    "nb_books = 25\n",
    "mode = \"chunks\"\n",
    "all_sentences = loader.load_books(\"Tom Swift\", mode, max_token_size, tokenizer=model.tokenizer)\n",
    "#Randomly reorder the books\n",
    "random.shuffle(all_sentences)\n",
    "sentences = all_sentences[:nb_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "district where our men were working, and now the privilege, or concession, has been withdrawn. I'm going down to see if I can't get it back. And I want you to go with me.\" \"And I came here for very nearly the same thing,\" went on Mr. Titus. \"That is where the coincidence comes in. It is strange that we should both appeal to Mr. Swift at the same time.\" \"Well, Tom's a valuable helper!\" exclaimed Mr. Damon. \"I know him of old, for I've been on many a trip with him.\" \"This is the first time I have had the pleasure of meeting him,\" resumed the tunnel contractor, \"but I have heard of him. I did not ask him to go to South America for us. I only wanted to get some superior explosive for my brother, who is in charge of driving the railroad tunnel through a spur of the Andes. I look after matters up North here, but I may have to go to Peru myself. \"As I told Mr. Swift, I had read of his invention of the giant cannon and the special powder he used in it to send a projectile such a distance. The cannon is now mounted as one of the pieces of ordnance for the defense of the Panama Canal, is it not?\" he asked Tom. The young inventor nodded in assent. \"Having heard of you, and the wonderful explosive used in your big cannon,\" the contractor went on, \"I wrote to my brother that I would try and get some for him. \"You see,\" he resumed, \"this is the situation. Back in the Andes Mountains, a couple of hundred miles east of Lima, the government is building a short railroad line to connect two others. If this is done it will mean that the products of Peru--quinine \n"
     ]
    }
   ],
   "source": [
    "#choose a random book and print 1 chunk\n",
    "r = random.randrange(0, len(sentences)) #not included\n",
    "r2 = random.randrange(0, len(sentences[r]))\n",
    "for i in range(r2, r2+1):\n",
    "    print(sentences[r][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Here we use Sentence-BERT to embed the sentences in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings of the sentences (1 min - 2 min)\n",
    "sentence_embedding = [model.encode(sentences[r]) for r in range(len(sentences))] #cannot stack because different number of pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing the distances\n",
    "Here we want to find an oprtimal order by maximizing the semantic proximity between neighboring sentences. We have $n!$ possible orderings, so we can't use brute force. Our problem is similar to the traveling salesman problem, which is NP-hard, so we can't solve it optimally, therefore we design some algorithms to try to find a local minimum instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<lambda>       avg_consecutive_dist     0.31866154074668884\n",
      "<lambda>       avg_swap_dist            0.0\n",
      "<lambda>       avg_R_dist               0.0\n",
      "\n",
      "<lambda>       avg_consecutive_dist     0.43418964743614197\n",
      "<lambda>       avg_swap_dist            0.249\n",
      "<lambda>       avg_R_dist               0.99\n",
      "\n",
      "insertion_sort avg_consecutive_dist     0.31866154074668884\n",
      "insertion_sort avg_swap_dist            0.0\n",
      "insertion_sort avg_R_dist               0.0\n",
      "\n",
      "insertion_sort avg_consecutive_dist     0.43418964743614197\n",
      "insertion_sort avg_swap_dist            0.249\n",
      "insertion_sort avg_R_dist               0.99\n",
      "\n",
      "greedy_sort    avg_consecutive_dist     0.31866154074668884\n",
      "greedy_sort    avg_swap_dist            0.0\n",
      "greedy_sort    avg_R_dist               0.0\n",
      "\n",
      "greedy_sort    avg_consecutive_dist     0.43418964743614197\n",
      "greedy_sort    avg_swap_dist            0.249\n",
      "greedy_sort    avg_R_dist               0.99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run the local minimum algorithms on a subset of the sentences of the book and compare the permutation distances (1 sec - 4 secs)\n",
    "pairwise_dist = distances.pairwise_dist(sentence_embedding)\n",
    "distances2 = pairwise_dist[0][:100,:100]\n",
    "\n",
    "default_order, random_order = list(range(len(distances2))), np.random.permutation(len(distances2))\n",
    "\n",
    "for algo in [(lambda x, y: y), algos.insertion_sort, algos.greedy_sort]:\n",
    "    for order in [default_order, random_order]:\n",
    "        for dist, dist_name in [(lambda o : distances.avg_consecutive_dist(o, distances2), \"avg_consecutive_dist\"), (lambda o : distances.avg_swap_dist(o, default_order), \"avg_swap_dist\"), (lambda o : distances.avg_R_dist(o, default_order), \"avg_R_dist\")]:\n",
    "            ordered = algo(distances2, order)\n",
    "            d = dist(order)\n",
    "            print(f\"{algo.__name__:15}{dist_name:25}{d}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first algorithm improves significantly the swap distance compared to a random permutation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier: \n",
    "### Heuristics\n",
    "The two metrics discussed were the following: distance betweeen pages or probability that a page is before another.\n",
    "\n",
    "For the distance between pages, we don't have the ground truth so to train a model it's easier to start with a classifier that takes in two pages and outputs a probability that the first page is before the second.\n",
    "\n",
    "![Classifier Architecture](../img/Classifier_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(279, 768)\n",
      "torch.Size([223, 768]) torch.Size([28, 768]) torch.Size([28, 768])\n"
     ]
    }
   ],
   "source": [
    "#split the sentences into discard, training, validation and testing sets keeping the order\n",
    "print(sentence_embedding[0].shape)\n",
    "sentences_train, sentences_val, sentences_test = dp.split_sentences(sentence_embedding, 1)\n",
    "print(sentences_train[0].shape, sentences_val[0].shape, sentences_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([864198, 1536]) torch.Size([13386, 1536]) torch.Size([13204, 1536])\n"
     ]
    }
   ],
   "source": [
    "#create the database of the pairs of a subset of sentences (30 sec - 1 min for 25 books and 100%)\n",
    "X_train, y_train = dp.create_database(sentences_train)\n",
    "X_val, y_val = dp.create_database(sentences_val)\n",
    "X_test, y_test = dp.create_database(sentences_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)     #size n x (n-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "input_dim = X_train[0].shape[0]\n",
    "output_dim = 1\n",
    "hidden_dim = 128\n",
    "learning_rate_list = [0.1, 0.01, 0.001, 0.0001]\n",
    "epochs_list = [10, 100, 1000]\n",
    "L2_alphas = [0, 0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the classifier\n",
    "network = classifier.Classifier(input_dim, hidden_dim, output_dim, accelerator)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#finetune the hyperparameters (10 minutes)\n",
    "best_loss, best_state, best_hyperparams = network.finetuning(learning_rate_list, epochs_list, L2_alphas, X_train, y_train, X_val, y_val, True)\n",
    "network.load_state_dict(best_state)\n",
    "print(\"best loss:\", best_loss)\n",
    "print(\"best learning rate:\", best_hyperparams[0])\n",
    "print(\"best epochs:\", best_hyperparams[1])\n",
    "print(\"best L2 alpha:\", best_hyperparams[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best values for the hyperparameters were found to be:\n",
    "- Number of epochs: 10 000\n",
    "- Learning Rate: 0.001\n",
    "- L2 Regularization: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the best hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "L2_alpha = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss 0.693072\n",
      "Epoch 100: train loss 0.535311\n",
      "Epoch 200: train loss 0.494067\n",
      "Epoch 300: train loss 0.448021\n",
      "Epoch 400: train loss 0.401572\n",
      "Epoch 500: train loss 0.361061\n",
      "Epoch 600: train loss 0.325222\n",
      "Epoch 700: train loss 0.293390\n",
      "Epoch 800: train loss 0.267515\n",
      "Epoch 900: train loss 0.242591\n",
      "Validation loss 0.560866\n"
     ]
    }
   ],
   "source": [
    "#train the network with the best hyperparameters (1 mins - 1.5 min)\n",
    "loss = network.train(X_train, y_train, X_val, y_val, epochs, learning_rate, L2_alpha, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values 13204\n",
      "Test BCE loss 0.480835\n",
      "Test accuracy 0.787034\n",
      "Test F1 0.787998\n",
      "Test AUC 0.787034\n"
     ]
    }
   ],
   "source": [
    "#test on the GPU\n",
    "y_pred = network(X_test.float().cuda())\n",
    "#BCE loss\n",
    "loss = network.loss_fn()(y_pred, y_test.float().cuda())\n",
    "#convert to numpy arrays and round predictions\n",
    "y_test_array, y_pred_array = y_test.cpu().detach().numpy(), y_pred.cpu().detach().numpy().round()\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y_test_array, y_pred_array)\n",
    "#F1 score\n",
    "F1 = f1_score(y_test_array, y_pred_array)\n",
    "#AUC score\n",
    "AUC = roc_auc_score(y_test_array, y_pred_array)\n",
    "\n",
    "print(\"Number of values %d\" % y_test_array.shape[0])\n",
    "print(\"Test BCE loss %f\" % loss.item())\n",
    "print(\"Test accuracy %f\" % accuracy)\n",
    "print(\"Test F1 %f\" % F1.item())\n",
    "print(\"Test AUC %f\" % AUC.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed a new book (2.5 sec)\n",
    "new_embedding = [model.encode(all_sentences[nb_books])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the pairwise page order in the new book using the network (2 secs)\n",
    "reduced_embedding = dp.split_sentences(new_embedding, 1, 1)[0]\n",
    "X2, y2 = dp.create_database(reduced_embedding)\n",
    "y_pred2 = network(X2.float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values 16512\n",
      "Test BCE loss 0.876174\n",
      "Test accuracy 0.576672\n",
      "Test F1 0.564323\n",
      "Test AUC 0.576672\n"
     ]
    }
   ],
   "source": [
    "#BCE loss\n",
    "loss = network.loss_fn()(y_pred2, y2.float().cuda())\n",
    "#convert to numpy arrays and round predictions\n",
    "y2_array, y_pred2_array = y2.cpu().detach().numpy(), y_pred2.cpu().detach().numpy().round()\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y2_array, y_pred2_array)\n",
    "#F1 score\n",
    "F1 = f1_score(y2_array, y_pred2_array)\n",
    "#AUC score\n",
    "AUC = roc_auc_score(y2_array, y_pred2_array)\n",
    "\n",
    "print(\"Number of values %d\" % y2_array.shape[0])\n",
    "print(\"Test BCE loss %f\" % loss.item())\n",
    "print(\"Test accuracy %f\" % accuracy)\n",
    "print(\"Test F1 %f\" % F1.item())\n",
    "print(\"Test AUC %f\" % AUC.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the predictions to a pairwise probability matrix\n",
    "pairwise_probabilities = dp.pred_to_pairwise(y_pred2)\n",
    "#compute the min weight transitivity closure of the pairwise order graph using Floyd-Warshall\n",
    "#compute obtimal order the topological sort algorithm\n",
    "pred_order = dp.weighted_transitivity_closure(pairwise_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swap distance: 0.031210\n",
      "R distance: 0.895652\n"
     ]
    }
   ],
   "source": [
    "#compare with the actual order of masked pages\n",
    "ground_truth_order = list(range(len(new_embedding[0])))\n",
    "print(\"Swap distance: %f\" % distances.avg_swap_dist(pred_order, ground_truth_order))\n",
    "print(\"R distance: %f\" % distances.avg_R_dist(pred_order, ground_truth_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Transformer:\n",
    "### Heuristics \n",
    "Full transformer network that takes in pages and outputs an order, the architecture for this set-to-sequence model comes from [Set Transformer](https://arxiv.org/abs/1810.00825).\n",
    "\n",
    "DRAWING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
