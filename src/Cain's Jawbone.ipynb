{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/.conda/envs/victor/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# global imports (1 min)\n",
    "import random\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\")\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports \n",
    "import loader\n",
    "import distances\n",
    "import algos\n",
    "import data_processing as dp\n",
    "import classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database\n",
    "\n",
    "The miscellaneous database consists of 20 copyright-free book from children's literature obtained from [The Project Gutenberg](https://www.gutenberg.org/ebooks/bookshelf/20).  \n",
    "The Tom Swift database consists of 27 books of the series Tom Swift by Victor Appleton obtained from [The Project Gutenberg](https://www.gutenberg.org/ebooks/search/?query=victor+appleton&submit_search=Go%21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing the database (\"Miscellaneous\", \"Tom Swift\") and the number of books to load\n",
    "nb_books = 3\n",
    "all_paragraphs, all_sentences = loader.load_books(\"Tom Swift\")\n",
    "paragraphs, sentences = all_paragraphs[:nb_books], all_sentences[:nb_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "way.\n",
      "\"Burn the house, boys!\" cried their officer; and this would be flashed\n",
      "on the screen later as a lead.\n",
      "The dwelling, which had been purchased with the right to burn it, was\n",
      "set afire, and then began a scene that satisfied even the exacting\n",
      "producer. Great clouds of smoke rolled out, most of it coming from\n",
      "specially prepared bombs, and amid them and the red fire, which\n",
      "simulated flames, could be seen the Union leader carrying out his\n",
      "sweetheart, Birdie Lee.\n",
      "Blake and Joe ground away at their cameras, faithfully recording the\n"
     ]
    }
   ],
   "source": [
    "#choose a random book and print 10 of its sentences\n",
    "r = random.randrange(0, len(sentences)) #not included\n",
    "for i in range(200, 210):\n",
    "    print(sentences[r][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Here we use Sentence-BERT to embed the sentences in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings of the sentences (1 min)\n",
    "sentence_embedding = [model.encode(sentences[r]) for r in range(len(sentences))] #cannot stack because different number of pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing the distances\n",
    "Here we want to find an oprtimal order by maximizing the semantic proximity between neighboring sentences. We have $n!$ possible orderings, so we can't use brute force. Our problem is similar to the traveling salesman problem, which is NP-hard, so we can't solve it optimally, therefore we design some algorithms to try to find a local minimum instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<lambda>       avg_consecutive_dist     0.758708119392395\n",
      "<lambda>       avg_swap_dist            0.0\n",
      "<lambda>       avg_R_dist               0.0\n",
      "\n",
      "<lambda>       avg_consecutive_dist     0.8512262105941772\n",
      "<lambda>       avg_swap_dist            0.26899999999999996\n",
      "<lambda>       avg_R_dist               0.97\n",
      "\n",
      "greedy_add     avg_consecutive_dist     0.758708119392395\n",
      "greedy_add     avg_swap_dist            0.0\n",
      "greedy_add     avg_R_dist               0.0\n",
      "\n",
      "greedy_add     avg_consecutive_dist     0.8512262105941772\n",
      "greedy_add     avg_swap_dist            0.26899999999999996\n",
      "greedy_add     avg_R_dist               0.97\n",
      "\n",
      "greedy_sort    avg_consecutive_dist     0.758708119392395\n",
      "greedy_sort    avg_swap_dist            0.0\n",
      "greedy_sort    avg_R_dist               0.0\n",
      "\n",
      "greedy_sort    avg_consecutive_dist     0.8512262105941772\n",
      "greedy_sort    avg_swap_dist            0.26899999999999996\n",
      "greedy_sort    avg_R_dist               0.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run the local minimum algorithms on a subset of the sentences of the book and compare the permutation distances\n",
    "pairwise_dist = distances.pairwise_dist(sentence_embedding)\n",
    "distances2 = pairwise_dist[0][:100,:100]\n",
    "\n",
    "default_order, random_order = list(range(len(distances2))), np.random.permutation(len(distances2))\n",
    "\n",
    "for algo in [(lambda x, y: y), algos.greedy_add, algos.greedy_sort]:\n",
    "    for order in [default_order, random_order]:\n",
    "        for dist, dist_name in [(lambda o : distances.avg_consecutive_dist(o, distances2), \"avg_consecutive_dist\"), (lambda o : distances.avg_swap_dist(o, default_order), \"avg_swap_dist\"), (lambda o : distances.avg_R_dist(o, default_order), \"avg_R_dist\")]:\n",
    "            ordered = algo(distances2, order)\n",
    "            d = dist(order)\n",
    "            print(f\"{algo.__name__:15}{dist_name:25}{d}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first algorithm improves significantly the swap distance compared to a random permutation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier: \n",
    "### Heuristics\n",
    "The two metrics discussed were the following: distance betweeen pages or probability that a page is before another\n",
    "For the distance between pages, we don't have the ground truth so to train a model it's easier to start with a classifier that takes in two pages and outputs a probability that the first page is before the second.\n",
    "\n",
    "![Classifier Architecture](../img/Classifier_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4036, 384)\n",
      "torch.Size([322, 384]) torch.Size([41, 384]) torch.Size([40, 384])\n"
     ]
    }
   ],
   "source": [
    "#split the sentences into discard, training, validation and testing sets keeping the order\n",
    "print(sentence_embedding[0].shape)\n",
    "sentences_train, sentences_val, sentences_test = dp.split_sentences(sentence_embedding)\n",
    "print(sentences_train[0].shape, sentences_val[0].shape, sentences_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/Jawbone/src/data_processing.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentence_embeddings = torch.tensor(sentence_embeddings).to(\"cuda\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([321150, 768]) torch.Size([5092, 768]) torch.Size([4848, 768])\n"
     ]
    }
   ],
   "source": [
    "#create the database of the pairs of a subset of sentences (45 secs for 3 books)\n",
    "X_train, y_train = dp.create_database(sentences_train)\n",
    "X_val, y_val = dp.create_database(sentences_val)\n",
    "X_test, y_test = dp.create_database(sentences_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)     #size n x (n-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "input_dim = X_train[0].shape[0]\n",
    "output_dim = 1\n",
    "hidden_dim = 128\n",
    "learning_rate_list = [0.1, 0.01, 0.001, 0.0001]\n",
    "epochs_list = [10, 100, 1000]\n",
    "L2_alphas = [0, 0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the classifier\n",
    "network = classifier.Classifier(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#finetune the hyperparameters (10 minutes)\n",
    "best_loss, best_state, best_hyperparams = network.finetuning(learning_rate_list, epochs_list, L2_alphas, X_train, y_train, X_val, y_val, True)\n",
    "network.load_state_dict(best_state)\n",
    "print(\"best loss:\", best_loss)\n",
    "print(\"best learning rate:\", best_hyperparams[0])\n",
    "print(\"best epochs:\", best_hyperparams[1])\n",
    "print(\"best L2 alpha:\", best_hyperparams[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best values for the hyperparameters were found to be:\n",
    "- Number of epochs: 10 000\n",
    "- Learning Rate: 0.001\n",
    "- L2 Regularization: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the best hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "L2_alpha = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss 0.693549\n",
      "Epoch 100: train loss 0.541392\n",
      "Epoch 200: train loss 0.481243\n",
      "Epoch 300: train loss 0.369160\n",
      "Epoch 400: train loss 0.255206\n",
      "Epoch 500: train loss 0.183871\n",
      "Epoch 600: train loss 0.140923\n",
      "Epoch 700: train loss 0.113037\n",
      "Epoch 800: train loss 0.093587\n",
      "Epoch 900: train loss 0.079201\n",
      "Validation loss 1.942119\n"
     ]
    }
   ],
   "source": [
    "#train the network with the best hyperparameters (4 mins)\n",
    "loss = network.train(X_train, y_train, X_val, y_val, epochs, learning_rate, L2_alpha, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BCE loss 1.632459\n",
      "Test accuracy 0.626238\n",
      "Test F1 0.624534\n",
      "Test AUC 0.626238\n"
     ]
    }
   ],
   "source": [
    "#test on the GPU\n",
    "y_pred = network(X_test.float().cuda())\n",
    "#BCE loss\n",
    "loss = network.loss_fn()(y_pred, y_test.float().cuda())\n",
    "#convert to numpy arrays and round predictions\n",
    "y_test_array, y_pred_array = y_test.cpu().detach().numpy(), y_pred.cpu().detach().numpy().round()\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y_test_array, y_pred_array)\n",
    "#F1 score\n",
    "F1 = f1_score(y_test_array, y_pred_array)\n",
    "#AUC score\n",
    "AUC = roc_auc_score(y_test_array, y_pred_array)\n",
    "\n",
    "print(\"Test BCE loss %f\" % loss.item())\n",
    "print(\"Test accuracy %f\" % accuracy)\n",
    "print(\"Test F1 %f\" % F1.item())\n",
    "print(\"Test AUC %f\" % AUC.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a new book\n",
    "new_embedding = [model.encode(all_sentences[nb_books])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/Jawbone/src/data_processing.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentence_embeddings = torch.tensor(sentence_embeddings).to(\"cuda\")\n"
     ]
    }
   ],
   "source": [
    "#compute the pairwise page order in the new book using the network (20 secs)\n",
    "reduced_embedding = dp.split_sentences(new_embedding, 0.1, 1)[0]\n",
    "X2, y2 = dp.create_database(reduced_embedding)\n",
    "y_pred2 = network(X2.float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BCE loss 2.800079\n",
      "Test accuracy 0.497549\n",
      "Test F1 0.499699\n",
      "Test AUC 0.497549\n"
     ]
    }
   ],
   "source": [
    "#BCE loss\n",
    "loss = network.loss_fn()(y_pred2, y2.float().cuda())\n",
    "#convert to numpy arrays and round predictions\n",
    "y2_array, y_pred2_array = y2.cpu().detach().numpy(), y_pred2.cpu().detach().numpy().round()\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y2_array, y_pred2_array)\n",
    "#F1 score\n",
    "F1 = f1_score(y2_array, y_pred2_array)\n",
    "#AUC score\n",
    "AUC = roc_auc_score(y2_array, y_pred2_array)\n",
    "print(\"Test BCE loss %f\" % loss.item())\n",
    "print(\"Test accuracy %f\" % accuracy)\n",
    "print(\"Test F1 %f\" % F1.item())\n",
    "print(\"Test AUC %f\" % AUC.item())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#deduce a local minimum ordering based in pairwise sentence orders in O(n^2)\n",
    "def greedy_add(order, permutation = []):\n",
    "    permutation = list(permutation)\n",
    "    #if no permutation is given, we create the default order\n",
    "    if len(permutation) == 0:\n",
    "        permutation = list(range(len(distances)))\n",
    "    ordered = []\n",
    "    for i in permutation:\n",
    "        #find the position that minimizes the sum of wrong orderings\n",
    "        n = len(ordered) \n",
    "        min = n\n",
    "        i_min = 0\n",
    "        for j in range(n+1):\n",
    "            #compute the number of wrong orderings if we insert the current sentence at position j\n",
    "            wrong = 0\n",
    "            for k in range(n):\n",
    "                if (order[ordered[k]] < order[i]) != (k < j):\n",
    "                    wrong += 1\n",
    "            if wrong < min:\n",
    "                min = wrong\n",
    "                i_min = j\n",
    "        #insert the current sentence in the ordered list\n",
    "        ordered.insert(i_min, i)\n",
    "    return(ordered)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the permutation distances to the real order\n",
    "#compute the real order of the masked sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer:\n",
    "### Heuristics \n",
    "Full transformer network that takes in pages as tokens and outputs an order, and the loss function would be a distance between two permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end-to-end transformer model using swap distance or R-distance as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nOrdering the pages of a book using a transformer model can be challenging, especially when dealing with limited token size. Here's how you can create a transformer model for this problem and address the token size limitation:\\n\\n1. Data Preparation:\\n\\n    Start by collecting a dataset of books with pages not in order. Each page should be a separate input example, and the pages should be represented as text.\\n\\n2. Text Tokenization:\\n\\n    Tokenize the text from each page into smaller units, such as words or subwords, using a tokenizer. Popular tokenization libraries like Hugging Face Transformers provide tokenizers that can handle large texts and split them into tokens without worrying about the token size limitation.\\n\\n3. Sliding Window Approach:\\n\\n    As you mentioned, most transformer models have a token size limitation. To overcome this limitation, you can use a sliding window approach. Split each page into overlapping segments or windows of tokens. This will allow you to work with manageable token sizes for your model.\\n\\n4. Model Architecture:\\n\\n    You can use a standard transformer architecture for this task. However, you might need to modify it slightly to account for the specific requirements of ordering pages. Your model should be capable of learning the relationships between pages and their optimal order.\\n\\n5. Training:\\n\\n    Train your transformer model on the dataset of disordered pages. You can use a contrastive loss function to ensure that the model learns to distinguish between correct and incorrect page orderings. This involves providing pairs of pages where one pair is in the correct order, and another pair is not.\\n\\n6. Inference:\\n\\n    When you want to order a book with disordered pages, you can feed the pages into your trained model. The model should provide a probability score or ranking for each possible page order. You can then select the order with the highest score as the predicted correct order.\\n\\n7. Evaluation:\\n\\n    To evaluate the model's performance, you can use metrics like mean squared error (MSE) or Kendall's Tau rank correlation to compare the predicted order with the ground truth order.\\n\\nKeep in mind that this is a challenging NLP task, and it may require a significant amount of data and computational resources. Additionally, your sliding window approach should be carefully designed to minimize information loss while breaking down the text into manageable token-sized chunks.\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ChatGPT approach:\n",
    "\"\"\"\n",
    "Ordering the pages of a book using a transformer model can be challenging, especially when dealing with limited token size. Here's how you can create a transformer model for this problem and address the token size limitation:\n",
    "\n",
    "1. Data Preparation:\n",
    "\n",
    "    Start by collecting a dataset of books with pages not in order. Each page should be a separate input example, and the pages should be represented as text.\n",
    "\n",
    "2. Text Tokenization:\n",
    "\n",
    "    Tokenize the text from each page into smaller units, such as words or subwords, using a tokenizer. Popular tokenization libraries like Hugging Face Transformers provide tokenizers that can handle large texts and split them into tokens without worrying about the token size limitation.\n",
    "\n",
    "3. Sliding Window Approach:\n",
    "\n",
    "    As you mentioned, most transformer models have a token size limitation. To overcome this limitation, you can use a sliding window approach. Split each page into overlapping segments or windows of tokens. This will allow you to work with manageable token sizes for your model.\n",
    "\n",
    "4. Model Architecture:\n",
    "\n",
    "    You can use a standard transformer architecture for this task. However, you might need to modify it slightly to account for the specific requirements of ordering pages. Your model should be capable of learning the relationships between pages and their optimal order.\n",
    "\n",
    "5. Training:\n",
    "\n",
    "    Train your transformer model on the dataset of disordered pages. You can use a contrastive loss function to ensure that the model learns to distinguish between correct and incorrect page orderings. This involves providing pairs of pages where one pair is in the correct order, and another pair is not.\n",
    "\n",
    "6. Inference:\n",
    "\n",
    "    When you want to order a book with disordered pages, you can feed the pages into your trained model. The model should provide a probability score or ranking for each possible page order. You can then select the order with the highest score as the predicted correct order.\n",
    "\n",
    "7. Evaluation:\n",
    "\n",
    "    To evaluate the model's performance, you can use metrics like mean squared error (MSE) or Kendall's Tau rank correlation to compare the predicted order with the ground truth order.\n",
    "\n",
    "Keep in mind that this is a challenging NLP task, and it may require a significant amount of data and computational resources. Additionally, your sliding window approach should be carefully designed to minimize information loss while breaking down the text into manageable token-sized chunks.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
