{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token size: 384\n"
     ]
    }
   ],
   "source": [
    "# global imports (1 sec - 30 secs)\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from sentence_transformers import SentenceTransformer\n",
    "accelerator = Accelerator()\n",
    "model = accelerator.prepare(SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda\")) #all-MiniLM-L6-v2\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "# Get the max token size\n",
    "max_token_size = model.max_seq_length\n",
    "print(f\"Max token size: {max_token_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports (1 sec - 2 secs)\n",
    "import loader\n",
    "import distances\n",
    "import algos\n",
    "import data_processing as dp\n",
    "import classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear all memory of the GPU\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.reset_peak_memory_stats(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database\n",
    "\n",
    "The miscellaneous database consists of 20 copyright-free book from children's literature obtained from [The Project Gutenberg](https://www.gutenberg.org/ebooks/bookshelf/20).  \n",
    "The Tom Swift database consists of 27 books of the series Tom Swift by Victor Appleton obtained from [The Project Gutenberg](https://www.gutenberg.org/ebooks/search/?query=victor+appleton&submit_search=Go%21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing the database (\"Miscellaneous\", \"Tom Swift\") and the number of books to load (1 min)\n",
    "nb_books = 25\n",
    "mode = \"chunks\"\n",
    "all_sentences = loader.load_books(\"Tom Swift\", mode, max_token_size, tokenizer=model.tokenizer)\n",
    "sentences = all_sentences[:nb_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Motor-Cycle,\" here was related how he came to possess that machine. A certain Mr. Wakefield Damon, an eccentric gentleman, who was always blessing himself, or something about him, owned the cycle, but he came to grief on it, and sold it to Tom very cheaply. Tom had a number of adventures on the wheel, and, after having used the motor to save a valuable patent model from a gang of unscrupulous men, the lad acquired possession of a power boat, in which he made several trips, and took part in many exciting happenings. Some time later, in company with John Sharp, an aeronaut, whom Tom had rescued from Lake Carlopa, after the airman had nearly lost his life in a burning balloon, the young inventor made a big airship, called the Red Cloud. With Mr. Damon, Tom made several trips in this craft, as set forth in the book, \"Tom Swift and His Airship.\" It was after this that Tom and his father built a submarine boat, and went under the ocean for sunken treasure, and, following that trip Tom built a speedy electric runabout, and by a remarkable run in that, with Mr. Damon, saved a bank from ruin, bringing gold in time to stave off a panic. \"Tom Swift and His Wireless Message\" told of the young inventor's plan to save the castaways of Earthquake Island, and how he accomplished it by constructing a wireless plant from the remains of the wrecked airship Whizzer. After Tom got back from Earthquake Island he went with Mr. Barcoe Jenks, whom he met on the ill-fated bit of land, to discover the secret of the diamond makers. They found the mysterious men, but the trip was not entirely successful, for the mountain containing the cave where the diamonds were made was destroyed by a lightning shock, just as Mr. \n"
     ]
    }
   ],
   "source": [
    "#choose a random book and print 1 chunk\n",
    "r = random.randrange(0, len(sentences)) #not included\n",
    "r2 = random.randrange(0, len(sentences[r]))\n",
    "for i in range(r2, r2+1):\n",
    "    print(sentences[r][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Here we use Sentence-BERT to embed the sentences in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings of the sentences (1 min - 2 min)\n",
    "sentence_embedding = [model.encode(sentences[r]) for r in range(len(sentences))] #cannot stack because different number of pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing the distances\n",
    "Here we want to find an oprtimal order by maximizing the semantic proximity between neighboring sentences. We have $n!$ possible orderings, so we can't use brute force. Our problem is similar to the traveling salesman problem, which is NP-hard, so we can't solve it optimally, therefore we design some algorithms to try to find a local minimum instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<lambda>       avg_consecutive_dist     0.3072151839733124\n",
      "<lambda>       avg_swap_dist            0.0\n",
      "<lambda>       avg_R_dist               0.0\n",
      "\n",
      "<lambda>       avg_consecutive_dist     0.4523981511592865\n",
      "<lambda>       avg_swap_dist            0.2351\n",
      "<lambda>       avg_R_dist               1.0\n",
      "\n",
      "insertion_sort avg_consecutive_dist     0.3072151839733124\n",
      "insertion_sort avg_swap_dist            0.0\n",
      "insertion_sort avg_R_dist               0.0\n",
      "\n",
      "insertion_sort avg_consecutive_dist     0.4523981511592865\n",
      "insertion_sort avg_swap_dist            0.2351\n",
      "insertion_sort avg_R_dist               1.0\n",
      "\n",
      "greedy_sort    avg_consecutive_dist     0.3072151839733124\n",
      "greedy_sort    avg_swap_dist            0.0\n",
      "greedy_sort    avg_R_dist               0.0\n",
      "\n",
      "greedy_sort    avg_consecutive_dist     0.4523981511592865\n",
      "greedy_sort    avg_swap_dist            0.2351\n",
      "greedy_sort    avg_R_dist               1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run the local minimum algorithms on a subset of the sentences of the book and compare the permutation distances (1 sec - 4 secs)\n",
    "pairwise_dist = distances.pairwise_dist(sentence_embedding)\n",
    "distances2 = pairwise_dist[0][:100,:100]\n",
    "\n",
    "default_order, random_order = list(range(len(distances2))), np.random.permutation(len(distances2))\n",
    "\n",
    "for algo in [(lambda x, y: y), algos.insertion_sort, algos.greedy_sort]:\n",
    "    for order in [default_order, random_order]:\n",
    "        for dist, dist_name in [(lambda o : distances.avg_consecutive_dist(o, distances2), \"avg_consecutive_dist\"), (lambda o : distances.avg_swap_dist(o, default_order), \"avg_swap_dist\"), (lambda o : distances.avg_R_dist(o, default_order), \"avg_R_dist\")]:\n",
    "            ordered = algo(distances2, order)\n",
    "            d = dist(order)\n",
    "            print(f\"{algo.__name__:15}{dist_name:25}{d}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first algorithm improves significantly the swap distance compared to a random permutation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier: \n",
    "### Heuristics\n",
    "The two metrics discussed were the following: distance betweeen pages or probability that a page is before another\n",
    "For the distance between pages, we don't have the ground truth so to train a model it's easier to start with a classifier that takes in two pages and outputs a probability that the first page is before the second.\n",
    "\n",
    "![Classifier Architecture](../img/Classifier_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152, 768)\n",
      "torch.Size([121, 768]) torch.Size([16, 768]) torch.Size([15, 768])\n"
     ]
    }
   ],
   "source": [
    "#split the sentences into discard, training, validation and testing sets keeping the order\n",
    "print(sentence_embedding[0].shape)\n",
    "sentences_train, sentences_val, sentences_test = dp.split_sentences(sentence_embedding, 1)\n",
    "print(sentences_train[0].shape, sentences_val[0].shape, sentences_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\Documents\\Scolaire\\EPFL 2022-2024\\Semester 3\\Jawbone\\src\\data_processing.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentence_embeddings = torch.tensor(sentence_embeddings).to(\"cuda\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([362852, 1536]) torch.Size([5732, 1536]) torch.Size([5316, 1536])\n"
     ]
    }
   ],
   "source": [
    "#create the database of the pairs of a subset of sentences (30 sec - 1 min for 25 books and 100%)\n",
    "X_train, y_train = dp.create_database(sentences_train)\n",
    "X_val, y_val = dp.create_database(sentences_val)\n",
    "X_test, y_test = dp.create_database(sentences_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)     #size n x (n-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "input_dim = X_train[0].shape[0]\n",
    "output_dim = 1\n",
    "hidden_dim = 128\n",
    "learning_rate_list = [0.1, 0.01, 0.001, 0.0001]\n",
    "epochs_list = [10, 100, 1000]\n",
    "L2_alphas = [0, 0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the classifier\n",
    "network = classifier.Classifier(input_dim, hidden_dim, output_dim, accelerator)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#finetune the hyperparameters (10 minutes)\n",
    "best_loss, best_state, best_hyperparams = network.finetuning(learning_rate_list, epochs_list, L2_alphas, X_train, y_train, X_val, y_val, True)\n",
    "network.load_state_dict(best_state)\n",
    "print(\"best loss:\", best_loss)\n",
    "print(\"best learning rate:\", best_hyperparams[0])\n",
    "print(\"best epochs:\", best_hyperparams[1])\n",
    "print(\"best L2 alpha:\", best_hyperparams[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best values for the hyperparameters were found to be:\n",
    "- Number of epochs: 10 000\n",
    "- Learning Rate: 0.001\n",
    "- L2 Regularization: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the best hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "L2_alpha = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss 0.693450\n",
      "Epoch 100: train loss 0.494742\n",
      "Epoch 200: train loss 0.452546\n",
      "Epoch 300: train loss 0.423044\n",
      "Epoch 400: train loss 0.380826\n",
      "Epoch 500: train loss 0.332385\n",
      "Epoch 600: train loss 0.289746\n",
      "Epoch 700: train loss 0.254822\n",
      "Epoch 800: train loss 0.226111\n",
      "Epoch 900: train loss 0.201926\n",
      "Validation loss 0.486506\n"
     ]
    }
   ],
   "source": [
    "#train the network with the best hyperparameters (1 mins - 1.5 min)\n",
    "loss = network.train(X_train, y_train, X_val, y_val, epochs, learning_rate, L2_alpha, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values 5316\n",
      "Test BCE loss 0.473978\n",
      "Test accuracy 0.799661\n",
      "Test F1 0.798028\n",
      "Test AUC 0.799661\n"
     ]
    }
   ],
   "source": [
    "#test on the GPU\n",
    "y_pred = network(X_test.float().cuda())\n",
    "#BCE loss\n",
    "loss = network.loss_fn()(y_pred, y_test.float().cuda())\n",
    "#convert to numpy arrays and round predictions\n",
    "y_test_array, y_pred_array = y_test.cpu().detach().numpy(), y_pred.cpu().detach().numpy().round()\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y_test_array, y_pred_array)\n",
    "#F1 score\n",
    "F1 = f1_score(y_test_array, y_pred_array)\n",
    "#AUC score\n",
    "AUC = roc_auc_score(y_test_array, y_pred_array)\n",
    "\n",
    "print(\"Number of values %d\" % y_test_array.shape[0])\n",
    "print(\"Test BCE loss %f\" % loss.item())\n",
    "print(\"Test accuracy %f\" % accuracy)\n",
    "print(\"Test F1 %f\" % F1.item())\n",
    "print(\"Test AUC %f\" % AUC.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed a new book (2.5 sec)\n",
    "new_embedding = [model.encode(all_sentences[nb_books])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the pairwise page order in the new book using the network (2 secs)\n",
    "reduced_embedding = dp.split_sentences(new_embedding, 1, 1)[0]\n",
    "X2, y2 = dp.create_database(reduced_embedding)\n",
    "y_pred2 = network(X2.float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values 24180\n",
      "Test BCE loss 0.629497\n",
      "Test accuracy 0.761497\n",
      "Test F1 0.763463\n",
      "Test AUC 0.761497\n"
     ]
    }
   ],
   "source": [
    "#BCE loss\n",
    "loss = network.loss_fn()(y_pred2, y2.float().cuda())\n",
    "#convert to numpy arrays and round predictions\n",
    "y2_array, y_pred2_array = y2.cpu().detach().numpy(), y_pred2.cpu().detach().numpy().round()\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y2_array, y_pred2_array)\n",
    "#F1 score\n",
    "F1 = f1_score(y2_array, y_pred2_array)\n",
    "#AUC score\n",
    "AUC = roc_auc_score(y2_array, y_pred2_array)\n",
    "\n",
    "print(\"Number of values %d\" % y2_array.shape[0])\n",
    "print(\"Test BCE loss %f\" % loss.item())\n",
    "print(\"Test accuracy %f\" % accuracy)\n",
    "print(\"Test F1 %f\" % F1.item())\n",
    "print(\"Test AUC %f\" % AUC.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the predictions to a pairwise probability matrix\n",
    "pairwise_probabilities = dp.pred_to_pairwise(y_pred2)\n",
    "#compute the min weight transitivity closure of the pairwise order graph using Floyd-Warshall\n",
    "#compute obtimal order the topological sort algorithm\n",
    "pred_order = dp.weighted_transitivity_closure(pairwise_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swap distance: 0.124877\n",
      "R distance: 0.897436\n"
     ]
    }
   ],
   "source": [
    "#compare with the actual order of masked pages\n",
    "ground_truth_order = list(range(len(new_embedding[0])))\n",
    "print(\"Swap distance: %f\" % distances.avg_swap_dist(pred_order, ground_truth_order))\n",
    "print(\"R distance: %f\" % distances.avg_R_dist(pred_order, ground_truth_order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer:\n",
    "### Heuristics \n",
    "Full transformer network that takes in pages as tokens and outputs an order, and the loss function would be a distance between two permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end-to-end transformer model using swap distance or R-distance as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nOrdering the pages of a book using a transformer model can be challenging, especially when dealing with limited token size. Here's how you can create a transformer model for this problem and address the token size limitation:\\n\\n1. Data Preparation:\\n\\n    Start by collecting a dataset of books with pages not in order. Each page should be a separate input example, and the pages should be represented as text.\\n\\n2. Text Tokenization:\\n\\n    Tokenize the text from each page into smaller units, such as words or subwords, using a tokenizer. Popular tokenization libraries like Hugging Face Transformers provide tokenizers that can handle large texts and split them into tokens without worrying about the token size limitation.\\n\\n3. Sliding Window Approach:\\n\\n    As you mentioned, most transformer models have a token size limitation. To overcome this limitation, you can use a sliding window approach. Split each page into overlapping segments or windows of tokens. This will allow you to work with manageable token sizes for your model.\\n\\n4. Model Architecture:\\n\\n    You can use a standard transformer architecture for this task. However, you might need to modify it slightly to account for the specific requirements of ordering pages. Your model should be capable of learning the relationships between pages and their optimal order.\\n\\n5. Training:\\n\\n    Train your transformer model on the dataset of disordered pages. You can use a contrastive loss function to ensure that the model learns to distinguish between correct and incorrect page orderings. This involves providing pairs of pages where one pair is in the correct order, and another pair is not.\\n\\n6. Inference:\\n\\n    When you want to order a book with disordered pages, you can feed the pages into your trained model. The model should provide a probability score or ranking for each possible page order. You can then select the order with the highest score as the predicted correct order.\\n\\n7. Evaluation:\\n\\n    To evaluate the model's performance, you can use metrics like mean squared error (MSE) or Kendall's Tau rank correlation to compare the predicted order with the ground truth order.\\n\\nKeep in mind that this is a challenging NLP task, and it may require a significant amount of data and computational resources. Additionally, your sliding window approach should be carefully designed to minimize information loss while breaking down the text into manageable token-sized chunks.\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ChatGPT approach:\n",
    "\"\"\"\n",
    "Ordering the pages of a book using a transformer model can be challenging, especially when dealing with limited token size. Here's how you can create a transformer model for this problem and address the token size limitation:\n",
    "\n",
    "1. Data Preparation:\n",
    "\n",
    "    Start by collecting a dataset of books with pages not in order. Each page should be a separate input example, and the pages should be represented as text.\n",
    "\n",
    "2. Text Tokenization:\n",
    "\n",
    "    Tokenize the text from each page into smaller units, such as words or subwords, using a tokenizer. Popular tokenization libraries like Hugging Face Transformers provide tokenizers that can handle large texts and split them into tokens without worrying about the token size limitation.\n",
    "\n",
    "3. Sliding Window Approach:\n",
    "\n",
    "    As you mentioned, most transformer models have a token size limitation. To overcome this limitation, you can use a sliding window approach. Split each page into overlapping segments or windows of tokens. This will allow you to work with manageable token sizes for your model.\n",
    "\n",
    "4. Model Architecture:\n",
    "\n",
    "    You can use a standard transformer architecture for this task. However, you might need to modify it slightly to account for the specific requirements of ordering pages. Your model should be capable of learning the relationships between pages and their optimal order.\n",
    "\n",
    "5. Training:\n",
    "\n",
    "    Train your transformer model on the dataset of disordered pages. You can use a contrastive loss function to ensure that the model learns to distinguish between correct and incorrect page orderings. This involves providing pairs of pages where one pair is in the correct order, and another pair is not.\n",
    "\n",
    "6. Inference:\n",
    "\n",
    "    When you want to order a book with disordered pages, you can feed the pages into your trained model. The model should provide a probability score or ranking for each possible page order. You can then select the order with the highest score as the predicted correct order.\n",
    "\n",
    "7. Evaluation:\n",
    "\n",
    "    To evaluate the model's performance, you can use metrics like mean squared error (MSE) or Kendall's Tau rank correlation to compare the predicted order with the ground truth order.\n",
    "\n",
    "Keep in mind that this is a challenging NLP task, and it may require a significant amount of data and computational resources. Additionally, your sliding window approach should be carefully designed to minimize information loss while breaking down the text into manageable token-sized chunks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
