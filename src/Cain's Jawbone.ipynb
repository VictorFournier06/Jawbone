{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 2.95MB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 420kB/s]\n",
      "README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 17.7MB/s]\n",
      "config.json: 100%|██████████| 571/571 [00:00<00:00, 1.23MB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 256kB/s]\n",
      "data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 35.4MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 438M/438M [00:01<00:00, 408MB/s] \n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 120kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 559kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 961kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 1.09MB/s]\n",
      "train_script.py: 100%|██████████| 13.1k/13.1k [00:00<00:00, 24.8MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.29MB/s]\n",
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 767kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token size: 384\n"
     ]
    }
   ],
   "source": [
    "# global imports\n",
    "import random\n",
    "import numpy as np\n",
    "from accelerate import Accelerator\n",
    "from sentence_transformers import SentenceTransformer\n",
    "accelerator = Accelerator()\n",
    "model = accelerator.prepare(SentenceTransformer(\"all-mpnet-base-v2\", device=\"cuda\")) #all-MiniLM-L6-v2\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "# Get the max token size\n",
    "max_token_size = model.max_seq_length\n",
    "print(f\"Max token size: {max_token_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports \n",
    "import loader\n",
    "import distances\n",
    "import algos\n",
    "import data_processing as dp\n",
    "import classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database\n",
    "\n",
    "The miscellaneous database consists of 20 copyright-free book from children's literature obtained from [The Project Gutenberg](https://www.gutenberg.org/ebooks/bookshelf/20).  \n",
    "The Tom Swift database consists of 27 books of the series Tom Swift by Victor Appleton obtained from [The Project Gutenberg](https://www.gutenberg.org/ebooks/search/?query=victor+appleton&submit_search=Go%21)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing the database (\"Miscellaneous\", \"Tom Swift\") and the number of books to load (1 min)\n",
    "nb_books = 25\n",
    "mode = \"chunks\"\n",
    "all_sentences = loader.load_books(\"Tom Swift\", mode, max_token_size, tokenizer=model.tokenizer)\n",
    "sentences = all_sentences[:nb_books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wouldn't make much of a moving picture. Well, if we go to Peru, we won't be far from the United States, and we can fly back home in the airship. But we've got to take the Flyer apart, and pack up again.\" \"Will you have time?\" asked Mr. Nestor. \"Maybe the volcano will get into action before you arrive, and the performance will be all over with.\" \"I think not,\" spoke Tom, as he again read the cablegram. \"Mr. Period says he has advices from Peru to the effect that, on other occasions, it took about a month from the time smoke was first seen coming from the crater, before the fireworks started up. I guess we've got time enough, but we won't waste any.\" \"And I guess Montgomery and Kenneth won't be there to make trouble for us,\" put in Ned. \"It will be some time before they get away from that African town, I think.\" They began work that day on taking the airship apart for transportation to the steamer that was to carry them across the ocean. Tom decided on going to Panama, to get a series of pictures on the work of digging that vast canal. On inquiry he learned that a steamer was soon to sail for Colon, so he took passage for his friends and himself on that, also arranging for the carrying of the parts of his airship. It was rather hard work to take the Flyer apart, but it was finally done, and, in about a week from the time of arriving in Paris, they left that beautiful city. The pictures already taken were forwarded to Mr. Period, with a letter of explanation of Tom's adventures thus far, and an account of how his rivals had acted. Just before sailing, Tom received another message from his strange employer. The cablegram \n"
     ]
    }
   ],
   "source": [
    "#choose a random book and print 1 chunk\n",
    "r = random.randrange(0, len(sentences)) #not included\n",
    "r2 = random.randrange(0, len(sentences[r]))\n",
    "for i in range(r2, r2+1):\n",
    "    print(sentences[r][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Here we use Sentence-BERT to embed the sentences in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings of the sentences (1 min)\n",
    "sentence_embedding = [model.encode(sentences[r]) for r in range(len(sentences))] #cannot stack because different number of pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing the distances\n",
    "Here we want to find an oprtimal order by maximizing the semantic proximity between neighboring sentences. We have $n!$ possible orderings, so we can't use brute force. Our problem is similar to the traveling salesman problem, which is NP-hard, so we can't solve it optimally, therefore we design some algorithms to try to find a local minimum instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<lambda>       avg_consecutive_dist     0.2835012674331665\n",
      "<lambda>       avg_swap_dist            0.0\n",
      "<lambda>       avg_R_dist               0.0\n",
      "\n",
      "<lambda>       avg_consecutive_dist     0.4096827208995819\n",
      "<lambda>       avg_swap_dist            0.2625\n",
      "<lambda>       avg_R_dist               1.0\n",
      "\n",
      "greedy_add     avg_consecutive_dist     0.2835012674331665\n",
      "greedy_add     avg_swap_dist            0.0\n",
      "greedy_add     avg_R_dist               0.0\n",
      "\n",
      "greedy_add     avg_consecutive_dist     0.4096827208995819\n",
      "greedy_add     avg_swap_dist            0.2625\n",
      "greedy_add     avg_R_dist               1.0\n",
      "\n",
      "greedy_sort    avg_consecutive_dist     0.2835012674331665\n",
      "greedy_sort    avg_swap_dist            0.0\n",
      "greedy_sort    avg_R_dist               0.0\n",
      "\n",
      "greedy_sort    avg_consecutive_dist     0.4096827208995819\n",
      "greedy_sort    avg_swap_dist            0.2625\n",
      "greedy_sort    avg_R_dist               1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run the local minimum algorithms on a subset of the sentences of the book and compare the permutation distances (1 sec)\n",
    "pairwise_dist = distances.pairwise_dist(sentence_embedding)\n",
    "distances2 = pairwise_dist[0][:100,:100]\n",
    "\n",
    "default_order, random_order = list(range(len(distances2))), np.random.permutation(len(distances2))\n",
    "\n",
    "for algo in [(lambda x, y: y), algos.greedy_add, algos.greedy_sort]:\n",
    "    for order in [default_order, random_order]:\n",
    "        for dist, dist_name in [(lambda o : distances.avg_consecutive_dist(o, distances2), \"avg_consecutive_dist\"), (lambda o : distances.avg_swap_dist(o, default_order), \"avg_swap_dist\"), (lambda o : distances.avg_R_dist(o, default_order), \"avg_R_dist\")]:\n",
    "            ordered = algo(distances2, order)\n",
    "            d = dist(order)\n",
    "            print(f\"{algo.__name__:15}{dist_name:25}{d}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first algorithm improves significantly the swap distance compared to a random permutation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier: \n",
    "### Heuristics\n",
    "The two metrics discussed were the following: distance betweeen pages or probability that a page is before another\n",
    "For the distance between pages, we don't have the ground truth so to train a model it's easier to start with a classifier that takes in two pages and outputs a probability that the first page is before the second.\n",
    "\n",
    "![Classifier Architecture](../img/Classifier_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 768)\n",
      "torch.Size([119, 768]) torch.Size([15, 768]) torch.Size([15, 768])\n"
     ]
    }
   ],
   "source": [
    "#split the sentences into discard, training, validation and testing sets keeping the order\n",
    "print(sentence_embedding[0].shape)\n",
    "sentences_train, sentences_val, sentences_test = dp.split_sentences(sentence_embedding, 1)\n",
    "print(sentences_train[0].shape, sentences_val[0].shape, sentences_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/Jawbone/src/data_processing.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentence_embeddings = torch.tensor(sentence_embeddings).to(\"cuda\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([366740, 1536]) torch.Size([5792, 1536]) torch.Size([5404, 1536])\n"
     ]
    }
   ],
   "source": [
    "#create the database of the pairs of a subset of sentences (30 secs for 25 books and 100%)\n",
    "X_train, y_train = dp.create_database(sentences_train)\n",
    "X_val, y_val = dp.create_database(sentences_val)\n",
    "X_test, y_test = dp.create_database(sentences_test)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)     #size n x (n-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "input_dim = X_train[0].shape[0]\n",
    "output_dim = 1\n",
    "hidden_dim = 128\n",
    "learning_rate_list = [0.1, 0.01, 0.001, 0.0001]\n",
    "epochs_list = [10, 100, 1000]\n",
    "L2_alphas = [0, 0.01, 0.001, 0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the classifier\n",
    "network = classifier.Classifier(input_dim, hidden_dim, output_dim, accelerator)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#finetune the hyperparameters (10 minutes)\n",
    "best_loss, best_state, best_hyperparams = network.finetuning(learning_rate_list, epochs_list, L2_alphas, X_train, y_train, X_val, y_val, True)\n",
    "network.load_state_dict(best_state)\n",
    "print(\"best loss:\", best_loss)\n",
    "print(\"best learning rate:\", best_hyperparams[0])\n",
    "print(\"best epochs:\", best_hyperparams[1])\n",
    "print(\"best L2 alpha:\", best_hyperparams[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best values for the hyperparameters were found to be:\n",
    "- Number of epochs: 10 000\n",
    "- Learning Rate: 0.001\n",
    "- L2 Regularization: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the best hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 1000\n",
    "L2_alpha = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss 0.693014\n",
      "Epoch 100: train loss 0.491927\n",
      "Epoch 200: train loss 0.454125\n",
      "Epoch 300: train loss 0.425547\n",
      "Epoch 400: train loss 0.386358\n",
      "Epoch 500: train loss 0.337621\n",
      "Epoch 600: train loss 0.291686\n",
      "Epoch 700: train loss 0.254032\n",
      "Epoch 800: train loss 0.223724\n",
      "Epoch 900: train loss 0.199020\n",
      "Validation loss 0.435678\n"
     ]
    }
   ],
   "source": [
    "#train the network with the best hyperparameters (1 mins)\n",
    "loss = network.train(X_train, y_train, X_val, y_val, epochs, learning_rate, L2_alpha, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values 5404\n",
      "Test BCE loss 0.451961\n",
      "Test accuracy 0.815507\n",
      "Test F1 0.814581\n",
      "Test AUC 0.815507\n"
     ]
    }
   ],
   "source": [
    "#test on the GPU\n",
    "y_pred = network(X_test.float().cuda())\n",
    "#BCE loss\n",
    "loss = network.loss_fn()(y_pred, y_test.float().cuda())\n",
    "#convert to numpy arrays and round predictions\n",
    "y_test_array, y_pred_array = y_test.cpu().detach().numpy(), y_pred.cpu().detach().numpy().round()\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y_test_array, y_pred_array)\n",
    "#F1 score\n",
    "F1 = f1_score(y_test_array, y_pred_array)\n",
    "#AUC score\n",
    "AUC = roc_auc_score(y_test_array, y_pred_array)\n",
    "\n",
    "print(\"Number of values %d\" % y_test_array.shape[0])\n",
    "print(\"Test BCE loss %f\" % loss.item())\n",
    "print(\"Test accuracy %f\" % accuracy)\n",
    "print(\"Test F1 %f\" % F1.item())\n",
    "print(\"Test AUC %f\" % AUC.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed a new book (2.5 sec)\n",
    "new_embedding = [model.encode(all_sentences[nb_books])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/Jawbone/src/data_processing.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentence_embeddings = torch.tensor(sentence_embeddings).to(\"cuda\")\n"
     ]
    }
   ],
   "source": [
    "#predict the pairwise page order in the new book using the network (2 secs)\n",
    "reduced_embedding = dp.split_sentences(new_embedding, 1, 1)[0]\n",
    "X2, y2 = dp.create_database(reduced_embedding)\n",
    "y_pred2 = network(X2.float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values 20880\n",
      "Test BCE loss 0.885846\n",
      "Test accuracy 0.651772\n",
      "Test F1 0.651889\n",
      "Test AUC 0.651772\n"
     ]
    }
   ],
   "source": [
    "#BCE loss\n",
    "loss = network.loss_fn()(y_pred2, y2.float().cuda())\n",
    "#convert to numpy arrays and round predictions\n",
    "y2_array, y_pred2_array = y2.cpu().detach().numpy(), y_pred2.cpu().detach().numpy().round()\n",
    "#accuracy\n",
    "accuracy = accuracy_score(y2_array, y_pred2_array)\n",
    "#F1 score\n",
    "F1 = f1_score(y2_array, y_pred2_array)\n",
    "#AUC score\n",
    "AUC = roc_auc_score(y2_array, y_pred2_array)\n",
    "\n",
    "print(\"Number of values %d\" % y2_array.shape[0])\n",
    "print(\"Test BCE loss %f\" % loss.item())\n",
    "print(\"Test accuracy %f\" % accuracy)\n",
    "print(\"Test F1 %f\" % F1.item())\n",
    "print(\"Test AUC %f\" % AUC.item())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#deduce a local minimum ordering based in pairwise sentence orders in O(n^2)\n",
    "def greedy_add(order, permutation = []):\n",
    "    permutation = list(permutation)\n",
    "    #if no permutation is given, we create the default order\n",
    "    if len(permutation) == 0:\n",
    "        permutation = list(range(len(distances)))\n",
    "    ordered = []\n",
    "    for i in permutation:\n",
    "        #find the position that minimizes the sum of wrong orderings\n",
    "        n = len(ordered) \n",
    "        min = n\n",
    "        i_min = 0\n",
    "        for j in range(n+1):\n",
    "            #compute the number of wrong orderings if we insert the current sentence at position j\n",
    "            wrong = 0\n",
    "            for k in range(n):\n",
    "                if (order[ordered[k]] < order[i]) != (k < j):\n",
    "                    wrong += 1\n",
    "            if wrong < min:\n",
    "                min = wrong\n",
    "                i_min = j\n",
    "        #insert the current sentence in the ordered list\n",
    "        ordered.insert(i_min, i)\n",
    "    return(ordered)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the permutation distances to the real order using an TSP - Traveling Salesman Problem - approximation algorithm\n",
    "\n",
    "#compute the real order of the masked sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer:\n",
    "### Heuristics \n",
    "Full transformer network that takes in pages as tokens and outputs an order, and the loss function would be a distance between two permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end-to-end transformer model using swap distance or R-distance as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nOrdering the pages of a book using a transformer model can be challenging, especially when dealing with limited token size. Here's how you can create a transformer model for this problem and address the token size limitation:\\n\\n1. Data Preparation:\\n\\n    Start by collecting a dataset of books with pages not in order. Each page should be a separate input example, and the pages should be represented as text.\\n\\n2. Text Tokenization:\\n\\n    Tokenize the text from each page into smaller units, such as words or subwords, using a tokenizer. Popular tokenization libraries like Hugging Face Transformers provide tokenizers that can handle large texts and split them into tokens without worrying about the token size limitation.\\n\\n3. Sliding Window Approach:\\n\\n    As you mentioned, most transformer models have a token size limitation. To overcome this limitation, you can use a sliding window approach. Split each page into overlapping segments or windows of tokens. This will allow you to work with manageable token sizes for your model.\\n\\n4. Model Architecture:\\n\\n    You can use a standard transformer architecture for this task. However, you might need to modify it slightly to account for the specific requirements of ordering pages. Your model should be capable of learning the relationships between pages and their optimal order.\\n\\n5. Training:\\n\\n    Train your transformer model on the dataset of disordered pages. You can use a contrastive loss function to ensure that the model learns to distinguish between correct and incorrect page orderings. This involves providing pairs of pages where one pair is in the correct order, and another pair is not.\\n\\n6. Inference:\\n\\n    When you want to order a book with disordered pages, you can feed the pages into your trained model. The model should provide a probability score or ranking for each possible page order. You can then select the order with the highest score as the predicted correct order.\\n\\n7. Evaluation:\\n\\n    To evaluate the model's performance, you can use metrics like mean squared error (MSE) or Kendall's Tau rank correlation to compare the predicted order with the ground truth order.\\n\\nKeep in mind that this is a challenging NLP task, and it may require a significant amount of data and computational resources. Additionally, your sliding window approach should be carefully designed to minimize information loss while breaking down the text into manageable token-sized chunks.\\n\""
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ChatGPT approach:\n",
    "\"\"\"\n",
    "Ordering the pages of a book using a transformer model can be challenging, especially when dealing with limited token size. Here's how you can create a transformer model for this problem and address the token size limitation:\n",
    "\n",
    "1. Data Preparation:\n",
    "\n",
    "    Start by collecting a dataset of books with pages not in order. Each page should be a separate input example, and the pages should be represented as text.\n",
    "\n",
    "2. Text Tokenization:\n",
    "\n",
    "    Tokenize the text from each page into smaller units, such as words or subwords, using a tokenizer. Popular tokenization libraries like Hugging Face Transformers provide tokenizers that can handle large texts and split them into tokens without worrying about the token size limitation.\n",
    "\n",
    "3. Sliding Window Approach:\n",
    "\n",
    "    As you mentioned, most transformer models have a token size limitation. To overcome this limitation, you can use a sliding window approach. Split each page into overlapping segments or windows of tokens. This will allow you to work with manageable token sizes for your model.\n",
    "\n",
    "4. Model Architecture:\n",
    "\n",
    "    You can use a standard transformer architecture for this task. However, you might need to modify it slightly to account for the specific requirements of ordering pages. Your model should be capable of learning the relationships between pages and their optimal order.\n",
    "\n",
    "5. Training:\n",
    "\n",
    "    Train your transformer model on the dataset of disordered pages. You can use a contrastive loss function to ensure that the model learns to distinguish between correct and incorrect page orderings. This involves providing pairs of pages where one pair is in the correct order, and another pair is not.\n",
    "\n",
    "6. Inference:\n",
    "\n",
    "    When you want to order a book with disordered pages, you can feed the pages into your trained model. The model should provide a probability score or ranking for each possible page order. You can then select the order with the highest score as the predicted correct order.\n",
    "\n",
    "7. Evaluation:\n",
    "\n",
    "    To evaluate the model's performance, you can use metrics like mean squared error (MSE) or Kendall's Tau rank correlation to compare the predicted order with the ground truth order.\n",
    "\n",
    "Keep in mind that this is a challenging NLP task, and it may require a significant amount of data and computational resources. Additionally, your sliding window approach should be carefully designed to minimize information loss while breaking down the text into manageable token-sized chunks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "victor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
